{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training LeNet using MNIST and Joey"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will construct and train LeNet using Joey, data from MNIST and the SGD with momentum PyTorch optimizer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with importing the prerequisites:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import joey as ml\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from devito import logger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to speed up processing, we'll not print performance messages coming from Devito."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.set_log_noperf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`create_lenet()` returns a `Net` instance representing LeNet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lenet():\n",
    "    # Six 3x3 filters, activation RELU\n",
    "    layer1 = ml.Conv(kernel_size=(6, 3, 3),\n",
    "                     input_size=(batch_size, 1, 32, 32),\n",
    "                     activation=ml.activation.ReLU())\n",
    "    # Max 2x2 subsampling\n",
    "    layer2 = ml.MaxPooling(kernel_size=(2, 2),\n",
    "                           input_size=(batch_size, 6, 30, 30),\n",
    "                           stride=(2, 2))\n",
    "    # Sixteen 3x3 filters, activation RELU\n",
    "    layer3 = ml.Conv(kernel_size=(16, 3, 3),\n",
    "                     input_size=(batch_size, 6, 15, 15),\n",
    "                     activation=ml.activation.ReLU())\n",
    "    # Max 2x2 subsampling\n",
    "    layer4 = ml.MaxPooling(kernel_size=(2, 2),\n",
    "                           input_size=(batch_size, 16, 13, 13),\n",
    "                           stride=(2, 2),\n",
    "                           strict_stride_check=False)\n",
    "    # Full connection (16 * 6 * 6 -> 120), activation RELU\n",
    "    layer5 = ml.FullyConnected(weight_size=(120, 576),\n",
    "                               input_size=(576, batch_size),\n",
    "                               activation=ml.activation.ReLU())\n",
    "    # Full connection (120 -> 84), activation RELU\n",
    "    layer6 = ml.FullyConnected(weight_size=(84, 120),\n",
    "                               input_size=(120, batch_size),\n",
    "                               activation=ml.activation.ReLU())\n",
    "    # Full connection (84 -> 10), output layer\n",
    "    layer7 = ml.FullyConnectedSoftmax(weight_size=(10, 84),\n",
    "                                      input_size=(84, batch_size))\n",
    "    # Flattening layer necessary between layer 4 and 5\n",
    "    layer_flat = ml.Flat(input_size=(batch_size, 16, 6, 6))\n",
    "    \n",
    "    layers = [layer1, layer2, layer3, layer4,\n",
    "              layer_flat, layer5, layer6, layer7]\n",
    "    \n",
    "    return (ml.Net(layers), layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A proper training iteration is carried out in `train()`. Note that we pass a PyTorch optimizer to `net.backward()`. Joey will take care to use it for updating weights appropriately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net, input_data, expected_results, pytorch_optimizer):\n",
    "    outputs = net.forward(input_data)\n",
    "    \n",
    "    def loss_grad(layer, expected):\n",
    "        gradients = []\n",
    "        \n",
    "        for b in range(len(expected)):\n",
    "            row = []\n",
    "            \n",
    "            for i in range(10):\n",
    "                result = layer.result.data[i, b]\n",
    "                if i == expected[b]:\n",
    "                    result -= 1\n",
    "                row.append(result)\n",
    "            \n",
    "            gradients.append(row)\n",
    "        \n",
    "        return gradients\n",
    "    \n",
    "    net.backward(expected_results, loss_grad, pytorch_optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, every batch will consist of 4 images and the training session will be capped at 100 iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "iterations = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before starting training, we need to download MNIST data using PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.Resize((32, 32)),\n",
    "     transforms.ToTensor(),\n",
    "     transforms.Normalize(0.5, 0.5)])\n",
    "trainset = torchvision.datasets.MNIST(root='./mnist', train=True, download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "\n",
    "classes = ('0', '1', '2', '3', '4', '5', '6', '7', '8', '9')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Afterwards, let's instantiate Joey's LeNet along with the SGD with momentum PyTorch optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/maksymilian/Desktop/UROP/devito/devito/types/grid.py:206: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  spacing = (np.array(self.extent) / (np.array(self.shape) - 1)).astype(self.dtype)\n"
     ]
    }
   ],
   "source": [
    "devito_net, devito_layers = create_lenet()\n",
    "optimizer = optim.SGD(devito_net.pytorch_parameters, lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're almost ready! The last thing to do is saving our original parameters as they will be required for making later comparisons with PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer1_kernel = torch.tensor(devito_layers[0].kernel.data)\n",
    "layer1_bias = torch.tensor(devito_layers[0].bias.data)\n",
    "layer3_kernel = torch.tensor(devito_layers[2].kernel.data)\n",
    "layer3_bias = torch.tensor(devito_layers[2].bias.data)\n",
    "layer5_kernel = torch.tensor(devito_layers[5].kernel.data)\n",
    "layer5_bias = torch.tensor(devito_layers[5].bias.data)\n",
    "layer6_kernel = torch.tensor(devito_layers[6].kernel.data)\n",
    "layer6_bias = torch.tensor(devito_layers[6].bias.data)\n",
    "layer7_kernel = torch.tensor(devito_layers[7].kernel.data)\n",
    "layer7_bias = torch.tensor(devito_layers[7].bias.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can start the Joey training session now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, data in enumerate(trainloader, 0):\n",
    "    images, labels = data\n",
    "    images.double()\n",
    "    \n",
    "    train(devito_net, images, labels, optimizer)\n",
    "    \n",
    "    if i == iterations - 1:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Afterwards, let's create a PyTorch equivalent of Joey's LeNet, train it using the same initial weights and data and compare the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 6, 3)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 3)\n",
    "        self.fc1 = nn.Linear(16 * 6 * 6, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
    "        x = x.view(-1, self.num_flat_features(x))\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "    \n",
    "    def num_flat_features(self, x):\n",
    "        size = x.size()[1:]\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net()\n",
    "net.double()\n",
    "\n",
    "with torch.no_grad():\n",
    "    net.conv1.weight[:] = layer1_kernel\n",
    "    net.conv1.bias[:] = layer1_bias\n",
    "    net.conv2.weight[:] = layer3_kernel\n",
    "    net.conv2.bias[:] = layer3_bias\n",
    "    net.fc1.weight[:] = layer5_kernel\n",
    "    net.fc1.bias[:] = layer5_bias\n",
    "    net.fc2.weight[:] = layer6_kernel\n",
    "    net.fc2.bias[:] = layer6_bias\n",
    "    net.fc3.weight[:] = layer7_kernel\n",
    "    net.fc3.bias[:] = layer7_bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "for i, data in enumerate(trainloader, 0):\n",
    "    images, labels = data\n",
    "    optimizer.zero_grad()\n",
    "    outputs = net(images.double())\n",
    "    loss = criterion(outputs, labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if i == iterations - 1:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layers[0] maximum relative error: 4.0927501282299136e-14\n",
      "layers[1] maximum relative error: 1.9990697131691112e-13\n",
      "layers[2] maximum relative error: 2.9535834223866145e-12\n",
      "layers[3] maximum relative error: 1.5112273951838733e-12\n",
      "layers[4] maximum relative error: 4.716492315053452e-11\n",
      "\n",
      "Maximum relative error is in layers[4]: 4.716492315053452e-11\n"
     ]
    }
   ],
   "source": [
    "layers = [devito_layers[0], devito_layers[2], devito_layers[5], devito_layers[6], devito_layers[7]]\n",
    "pytorch_layers = [net.conv1, net.conv2, net.fc1, net.fc2, net.fc3]\n",
    "\n",
    "max_error = 0\n",
    "index = -1\n",
    "\n",
    "for i in range(5):\n",
    "    kernel = layers[i].kernel.data\n",
    "    pytorch_kernel = pytorch_layers[i].weight.detach().numpy()\n",
    "    \n",
    "    kernel_error = abs(kernel - pytorch_kernel) / abs(pytorch_kernel)\n",
    "    \n",
    "    bias = layers[i].bias.data\n",
    "    pytorch_bias = pytorch_layers[i].bias.detach().numpy()\n",
    "    \n",
    "    bias_error = abs(bias - pytorch_bias) / abs(pytorch_bias)\n",
    "    \n",
    "    error = max(np.nanmax(kernel_error), np.nanmax(bias_error))\n",
    "    print('layers[' + str(i) + '] maximum relative error: ' + str(error))\n",
    "    \n",
    "    if error > max_error:\n",
    "        max_error = error\n",
    "        index = i\n",
    "\n",
    "print()\n",
    "print('Maximum relative error is in layers[' + str(index) + ']: ' + str(max_error))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the maximum relative error is low enough to consider the training session in Joey numerically correct."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
